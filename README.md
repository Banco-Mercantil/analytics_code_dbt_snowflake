# Primeiros passos com Snowflake e DBT Cloud:

O real objetivo deste trabalho √© demonstrar, passo a passo, como utilizar o DBT Cloud (Data Build Tool) e o SNOWFLAKE em um projeto anal√≠tico de dados.
  
SNOWFLAKE √© a nuvem de dados que permite criar aplicativos com uso intensivo de dados sem sobrecarga operacional, para que voc√™ possa se concentrar em dados e an√°lises em vez de no gerenciamento de infraestrutura.

DBT √© um fluxo de trabalho de transforma√ß√£o de dados seguindo as melhores pr√°ticas de engenharia de software. Utilizando o SQL √© poss√≠vel construir pipelines de dados de n√≠vel de produ√ß√£o. Ele transforma os dados no warehouse aproveitando plataformas de dados em nuvem como o SNOWFLAKE.


## üß™ Manual de Apoio
O projeto foi desenvolvido com base nas instru√ß√µes dos seguintes materiais:

- [Manual Base](https://quickstarts.snowflake.com/guide/accelerating_data_teams_with_snowflake_and_dbt_cloud_hands_on_lab/index.html?index=..%2F..index#0)

- [V√≠deo de apoio](https://www.youtube.com/watch?v=84RA7TuhCpg)

## üî® Ferramentas Necess√°rias
Para iniciar um projeto utilizando o DBT CLOUD, usaremos uma conta de teste. A cria√ß√£o da conta DBT CLOUD ser√° feita por meio do SNOWFLAKE atrav√©s de uma conex√£o nativa (parceira). 
 
Por meio desta conta, ser√° poss√≠vel construir pipelines de transforma√ß√£o de dados escal√°veis usando o DBT e o SNOWFLAKE.

- [Link de cria√ß√£o conta teste Snowflake](https://signup.snowflake.com/)

## üöÄ Inicializando o projeto DBT
Com a conta teste criada no Snowflake √© preciso verificar que possuimos todos os dados de que precisamos para transformar e executar o projeto. 

V√° para a guia **Worksheets**, clique na op√ß√£o de **+Worksheets** para adicionar uma nova planilha. Neste momento, clique em **Databases**, voc√™ ver√° dois banco de dados e um deles se chama **snowflake sample data**. Ao explandi-l√≥  voc√™ ver√° o schema **tpch sf1** e a partir da√≠ tem-se todas as tabelas que possuem os dados brutos que ser√£o transformados pelo projeto.

Para ter certeza, vamos exibir alguns resultados destas tabelas. Crie um **warehouse** de amostra e com a capacidade de tamanho pequena com a seguinte linha de comando:
  
```bash
create warehouse sample_warehouse with warehouse_size = xsmall
```

No quadro superior a direita, certifique de indicar a role **ACCOUNTADMIN** e o warehouse **SAMPLE_WAREHOUSE**, em seguida, execute o seguinte trecho de comando: 
  
```bash
select *
from snowflake_sample_data.tpch_sf1.orders
limit 100;
```
Os resultados ser√£o exibidos na sequ√™ncia.

Com isso, √© poss√≠vel criar uma conta  DBT Labs atrav√©s da partner connect. Pesquise por DBT em partner connect e selecione o item da pesquisa, no pop-up aberto, na se√ß√£o **Optional Grant** informe o nome do banco de dados criado **SNOWFLAKE_SAMPLE_DATA**. Siga com a conex√£o e ative-a.

Voc√™ ser√° redirecionado para o site do DBT. Crie uma conta neste ambiente, clicando em **Complete Registration**. Agora voc√™ deve ser redirecionado para sua conta DBT Cloud j√° com uma conex√£o com sua conta SNOWFLAKE.

Nesta etapa, para entrar na √°rea de nuvem DBT, abra a IDE clicando no bot√£o **Develop** no canto superior a esquerda. Quando o IDE terminar de carregar, clique no **Initialize dbt project**, bot√£o verde no canto superior esquerdo da tela. O processo de inicializa√ß√£o cria um projeto DBT na √°rvore de arquivos no lado esquerdo da tela com todos os arquivos e pastas principais do DBT necess√°rios. 

Ap√≥s o projeto criado, clique **Commit and sync**, no canto superior esquerdo, para enviar/submeter os novos arquivos e pastas da etapa de inicializa√ß√£o. Um pop-up ser√° aberto, informe uma mensagem de commit relevante para o trabalho: **initialize project**. Na sequencia, clique em **Commit Changes** na janela pop-up.

_Commit criar√° uma nova vers√£o com as edi√ß√µes acrescentadas ao projeto, gerando uma subvers√£o do projeto. Push ir√° direcionar essa altera√ß√£o da branch master para origin, ou seja, retirar do reposit√≥rio local (Git) para o reposit√≥rio remoto (GitHub)._
 
_Cada Commit dado, ser√° criada uma nova vers√£o, ao final do dia, d√°-se o push para gerar uma c√≥pia do projeto no GitHub (Origin)._

Ao confirmar, ele ser√° salvo no reposit√≥rio git gerenciado que foi criado durante a inscri√ß√£o no Partner Connect. Este commit inicial √© o √∫nico commit que ser√° feito diretamente em nosso main branch (branch master/ramifica√ß√£o principal) e daqui em diante estaremos fazendo todo o nosso trabalho em um branch de desenvolvimento (merge no master). Isso nos permite manter nosso trabalho de desenvolvimento separado do c√≥digo de produ√ß√£o.

A IDE possui alguns recursos chaves, ela √© um editor de texto, um executor SQL e uma CLI com controle de vers√£o git, tudo integrado em um √∫nico pacote. O fluxo de trabalho git no DBT Cloud permite controlar facilmente a vers√£o de todo o seu trabalho com apenas alguns cliques. 

Clique em **+Create new file** para abrir o notebook da IDE. Nesta fase, um SQL Runner √© aberto, uma linha de comando inferior na tela para executar comandos do DBT como ```dbt run``` e uma √°rvore de arquivos e controles git no canto superior a esquerda da tela.

![Imagem](https://quickstarts.snowflake.com/guide/accelerating_data_teams_with_snowflake_and_dbt_cloud_hands_on_lab/img/516d62e53cdc1ba1.png)

Agora √© poss√≠vel executar os primeiros modelos DBTs, os quais vem por padr√£o na pasta **models\example**, para ilustrar como executar o dbt na linha de comando. Digite ```dbt run``` na linha de comando na parte inferior da tela e pressione **Enter** no teclado. Quando a barra de execu√ß√£o se expandir, voc√™ poder√° ver os resultados da execu√ß√£o concluidos com sucesso.

![Imagem](https://quickstarts.snowflake.com/guide/accelerating_data_teams_with_snowflake_and_dbt_cloud_hands_on_lab/img/7c1cb782586306f4.png)

Os resultados da execu√ß√£o permitem que voc√™ veja o c√≥digo que o DBT compila e envia ao SNOWFLAKE para execu√ß√£o. Para visualizar os logs, clique em uma das guias do modelo e, em seguida, clique em **details**. Se voc√™ rolar um pouco para baixo, poder√° ver o c√≥digo compilado e como o DBT interage com o SNOWFLAKE. Dado que esta execu√ß√£o ocorreu em nosso ambiente de desenvolvimento, os modelos foram criados em seu esquema de desenvolvimento, estruturado como sua inicial e sobrenome:
```
PC_DBT_DB.dbt_PFernandes.my_first_dbt_model
PC_DBT_DB.dbt_PFernandes.my_second_dbt_model
```

Agora vamos passar para o Snowflake para confirmar se os objetos foram realmente criados. Retorne ao SNOWFLAKE, em Data -> Databases, clique em **refresh** e des√ßa a √°rvore do bando de dados **PC_DBT_DB**. Neste, dever√° haver o schema **dbt_PFernandes** que, por sua vez, dever√° conter as duas tabelas criadas: **my_first_dbt_model** e **my_second_dbt_model**.

Agora, no worksheet do Snowflake, crie um novo **warehouse** que usaremos no projeto dbt. Copie e cole a seguinte s√©rie de comandos e execute-os em ordem no worksheet do Snowflake.

```
	use role accountadmin;
	create warehouse pc_dbt_wh_large with warehouse_size = large;
	grant all on warehouse pc_dbt_wh_large to role pc_dbt_role;
```

Aqui estamos usando a fun√ß√£o de administrador de conta para criar um tamanho de **warehouse** maior e concedendo todas as habilidades para ler, escrever e todas as demais atribui√ß√µes.

Ap√≥s executar, no quadro superior a direita, certifique de indicar a role **ACCOUNTADMIN** e o warehouse **PC_DBT_WH_LARGE**.

Podemos, ent√£o, seguir para a pr√≥xima etapa de definir a estrutura fundamental do projeto. 

Para definir a base para este projeto vamos retornar ao **DBT Cloud** e criar um novo **branch** (ramifica√ß√£o). Clique no **create branch**, um bot√£o verde no canto superior esquerdo da tela para abrir uma janela e nomear sua filial (snowflake_workshop). Nesta fase, estaremos trabalhando no **branch** de desenvolvimento. 

O pr√≥ximo passo √© abrir o arquivo **dbt_project.yml** do projeto. Este √© o arquivo que o dbt procura para reconhecer que o diret√≥rio de arquivos em que estamos trabalhando √© um projeto DBT. Nele, vamos atualizar seus par√¢metros substituindo quase todo o conte√∫do existente, deixando apenas o trecho: 

```
# In dbt, the default materialization for a model is a view. This means, when you run 
# dbt run or dbt build, all of your models will be built as a view in your data platform. 
# The configuration below will override this setting for models in the example folder to 
# instead be materialized as tables. Any models you add to the root of the models folder will 
# continue to be built as views. These settings can be overridden in the individual model files
# using the `{{ config(...) }}` macro.

models:
  my_new_project:
    # Applies to all files under models/example/
		example:
		  materialized: view
```

Copie e cole o seguinte c√≥digo acima do trecho que devemos manter no arquivo:

```
name: 'snowflake_workshop'
version: '1.0.0'
config-version: 2

profile: 'default'

source-paths: ["models"] # A MINHA VERS√ÉO EXIGIU ESSA CONFIGURA√á√ÉO model_paths: ["models"]
analysis-paths: ["analysis"] # A MINHA VERS√ÉO EXIGIU ESSA CONFIGURA√á√ÉO analysis-paths: ["analyses"]
test-paths: ["tests"]
seed-paths: ["seeds"]
macro-paths: ["macros"]
snapshot-paths: ["snapshots"]

target-path: "target"  
clean-targets:         
  - "target"
  - "dbt_modules"

models:
  snowflake_workshop:
    staging:
      materialized: view
      snowflake_warehouse: pc_dbt_wh
    marts:
      materialized: table
      snowflake_warehouse: pc_dbt_wh_large
```

As principais configura√ß√µes no arquivo est√£o na se√ß√£o de modelos. Aqui estamos definindo configura√ß√µes que queremos aplicar a todos os modelos dentro dessa pasta. Especificamente, estamos demonstrando:

_a configura√ß√£o de **materialized**, que informa ao dbt como materializar modelos ao compilar o c√≥digo antes de envi√°-lo para o Snowflake, e
a configura√ß√£o do **snowflake_warehouse**, que especifica qual warehouse do Snowflake deve ser usado ao construir modelos no Snowflake._

A materializa√ß√£o define a estrat√©gia que o _DBT_ dever√° usar ao criar as tabela dentro do Snowflake. Geralmente s√£o **table** e **view** as mais comuns. Por padr√£o, todos os modelos s√£o materializados como **view** e outros tipos de visualiza√ß√µes podem ser configuradas no arquivo _.yaml_ do projeto.

O benef√≠cio de usar materializa√ß√µes _dbt_ √© que quando voc√™ executa um modelo _dbt_, o _dbt_ usar√° o _DDL_ correto associado √† materializa√ß√£o para criar o equivalente do modelo no data warehouse.

Cada conex√£o _dbt_ declara um _warehouse_ padr√£o para usar ao construir modelos no Snowflake. Com a configura√ß√£o **snowflake_warehouse**, o _dbt_ usar√° o warehouse x-small **pc_dbt_wh** para construir tudo na pasta de teste e o warehouse large **pc_dbt_wh_large** para construir tudo na pasta marts. Essa configura√ß√£o nos permite aumentar e diminuir o tamanho do warehouse com base nas necessidades de computa√ß√£o de diferentes partes do projeto.

A pr√≥xima fase iremos criar uma estrutura de pastas dentro da pasta **models**, clicando nos tr√™s pontinhos **Create Folder**. Digite ``staging/tpch`` e a pasta **staging** ser√° criada, dentro dela a pasta **tpch** que armazenar√° os dados TPCH.

![image](https://github.com/Banco-Mercantil/analytics_code_dbt_snowflake/assets/88452990/a33d922e-877f-41b0-9022-6232cf655380)

![image](https://github.com/Banco-Mercantil/analytics_code_dbt_snowflake/assets/88452990/0d91054a-9959-4b93-b6fb-2001401917ed)

Vamos criar nossas duas pastas finais da mesma maneira. Passe o cursor sobre o subdiret√≥rio **models**, clique nos tr√™s pontos que aparecem √† direita e clique em **Create Folder**. Desta vez, na janela pop-up, voc√™ inserir√° o seguinte caminho de arquivo: ```marts/core```. Aqui estamos criando uma pasta **marts** dentro **models** e depois uma pasta **core** dentro de **marts**. Sua √°rvore de pastas deve ficar assim quando tudo estiver dito e feito: 

![image](https://github.com/Banco-Mercantil/analytics_code_dbt_snowflake/assets/88452990/b176bbdb-6903-47d1-b3fc-dcf9ddb4cb73)

Essas pastas ser√£o usadas para armazenar os modelos de transforma√ß√£o e nossa prepara√ß√£o. 

A √∫ltima coisa que faremos para configurar o projeto _DBT_ √© implementar pacotes. **dbt packages** s√£o projetos _DBT_ que voc√™ pode trazer para o seu projeto e usar como se fossem seus. Muitos pacotes s√£o hospedados no **dbt Packages Hub** e fornecem a capacidade de gerar c√≥digo reutiliz√°vel para criar fontes e modelos _DBT_, acessar uma variedade de macros √∫teis em **jinja** e criar restri√ß√µes de banco de dados para seus bancos Snowflake.

_Jinja √© uma linguagem de modelagem python que voc√™ pode usar no dbt para fazer coisas que normalmente n√£o s√£o poss√≠veis com SQL. Alguns exemplos incluem a constru√ß√£o de estruturas de controle como ```if``` (instru√ß√µes) e ```for``` (loops) e a abstra√ß√£o de trechos de c√≥digo em macros reutiliz√°veis ‚Äã‚Äãpara que voc√™ n√£o se repita (DRY - Don't Repeat Yourself)._

_Uma macro √© um peda√ßo de c√≥digo jinja que pode ser reutilizado v√°rias vezes em um projeto dbt, como uma fun√ß√£o em outras linguagens de programa√ß√£o. Algumas das fun√ß√µes mais importantes do dbt, como ```source``` e  ```ref```, tamb√©m s√£o macros (abordaremos isso mais tarde no laborat√≥rio)._

Vamos adicionar o pacote **dbt_utils** ao projeto. H√° alguns lugares onde usaremos macros do pacote **dbt_utils** para nos ajudar a escrever c√≥digo DRY. Para instalar o pacote, primeiro crie um novo arquivo em seu diret√≥rio inicial (mesmo n√≠vel do seu arquivo **dbt_project.yml**) e chame-o de **packages.yml**. Em seguida, copie e cole o c√≥digo nele:

```
packages:
	- package: dbt-labs/dbt_utils
	version: 0.8.4
```

Salve o arquivo. Neste momento o DBT executar√° automaticamente o dbt deps, caso ele n√£o fa√ßa, execute o trecho ```dbt deps``` na linha de comando.

Essa a√ß√£o que informa ao _DBT_ para instalar os pacotes definidos em seu arquivo **packages.yml**. Clique em "Enter" e voc√™ ver√° uma mensagem de sucesso quando for conclu√≠do.

At√© aqui estamos desenvolvendo no reposit√≥rio local, vamos reservar um segundo aqui para submeter o trabalho antes de passar para a pr√≥xima etapa. Clique no bot√£o **Commit and sync**, escreva uma mensagem de commit como **setup structure for project** e clique em **Commit Changes** para confirmar as mudan√ßas feitas.

Na pr√≥xima etapa, usaremos as tabelas do Snowflake **orders** e **lineitem** do esquema **TPCH_SF1** para nossas transforma√ß√µes e vamos criar essas tabelas como fontes no projeto.

_Fontes: As fontes em dbt permitem nomear e descrever dados brutos de seu warehouse em seu projeto dbt, permitindo estabelecer a linhagem de dados brutos para modelos transformados._

_Modelos de teste: Os modelos de preparo t√™m um relacionamento 1:1 com tabelas de origem. √â aqui que voc√™ realizar√° transforma√ß√µes simples, como renomear, reformular ou alterar uma coluna para um formato mais utiliz√°vel. A constru√ß√£o de modelos de prepara√ß√£o fornece uma base clara para a modelagem downstream e permite a modelagem modular DRY._

Crie um novo arquivo chamado **tpch_sources.yml** com o seguinte caminho de arquivo: ``models/staging/tpch/tpch_sources.yml``. Este arquivo ser√° usado para definirmos nossas fontes do projeto, as tabelas  **orders** e **lineitem**. Em seguida, cole o seguinte c√≥digo no arquivo antes de salv√°-lo:

```
version: 2
sources:
	- name: tpch
	description: source tpch data
	database: snowflake_sample_data
	schema: tpch_sf1
	tables:
	  - name: orders
		description: main order tracking table
		columns:
		  - name: o_orderkey
			description: SF*1,500,000 are sparsely populated
			tests: 
			  - unique
			  - not_null

		  - name: lineitem
			description: main lineitem table
			columns:
			  - name: l_orderkey
				description: Foreign Key to O_ORDERKEY
				tests:
				  - relationships:
					  to: source('tpch', 'orders')
					  field: o_orderkey
	
```

No arquivo, voc√™ pode ver que definimos o banco de dados **snowflake_sample_data** de onde iremos obter as informa√ß√µes, o esquema **tpch_sf1** e ambas as tabelas com as quais iremos construir e transformar. Abaixo de cada nome de tabela temos descri√ß√µes e testes que aplicamos √†s nossas fontes.

Agora que temos as fontes instaladas, o que vamos fazer √© criar e configurar os **modelos de teste** para as duas fontes de dados. Estes ser√£o arquivos _.sql_. Dado o relacionamento individual entre os modelos de teste e suas tabelas de origem correspondentes, construiremos dois modelos de preparo.

Vamos come√ßar com a tabela de pedidos **(orders)**. Crie um novo arquivo chamado **stg_tpch_orders.sql** com o seguinte caminho de arquivo: _models/staging/tpch/stg_tpch_orders.sql_. Em seguida, cole o seguinte c√≥digo no arquivo antes de salv√°-lo:

```
with source as (
    select * from {{ source('tpch', 'orders') }}
),

renamed as (
    select
        o_orderkey as order_key,
        o_custkey as customer_key,
        o_orderstatus as status_code,
        o_totalprice as total_price,
        o_orderdate as order_date,
        o_orderpriority as priority_code,
        o_clerk as clerk_name,
        o_shippriority as ship_priority,
        o_comment as comment
    from source
)

select * from renamed
```

Tudo o que estamos fazendo aqui √© puxar os dados de origem para o modelo usando a fun√ß√£o **source** e renomear algumas colunas. Isso serve como ponto de partida para todos os outros modelos que precisam fazer refer√™ncia a esses dados para que a nomenclatura permane√ßa consistente em todo o projeto.

Para o segundo modelo, criaremos outro arquivo na mesma pasta chamada **stg_tpch_line_items.sql** com o seguinte caminho de arquivo: _models/staging/tpch/stg_tpch_line_items.sql_. Em seguida, cole o seguinte c√≥digo nele antes de salvar o arquivo:

```
with source as (
    select * from {{ source('tpch', 'lineitem') }}
),

renamed as (
    select  
        {{dbt_utils.surrogate_key (['l_orderkey', 'l_linenumber'])}} as order_item_key,
        l_orderkey as order_key,
        l_partkey as part_key,
        l_suppkey as supplier_key,
        l_linenumber as line_number,
        l_quantity as quantity,
        l_extendedprice as extended_price,
        l_discount as discount_percentage,
        l_tax as tax_rate,
        l_returnflag as return_flag,
        l_linestatus as status_code,
        l_shipdate as ship_date,
        l_commitdate as commit_date,
        l_receiptdate as receipt_date,
        l_shipinstruct as ship_instructions,
        l_shipmode as ship_mode,
        l_comment as comment
    from source
)

select * from renamed
```

Aqui temos uma renomea√ß√£o semelhante ao que fizemos no primeiro **modelo de teste**, bem como a adi√ß√£o de uma nova coluna utilizando o pacote **dbt_utils**. O **dbt_utils.surrogate_key** cria uma chave substituta com hash que chamamos **order_item_key** usando as colunas listadas na macro. A implementa√ß√£o dessa chave substituta nos d√° uma coluna exclusiva que podemos usar para testar facilmente a exclusividade posteriormente.

Ambos arquivos modelos criados, ser√£o a base de todos os demais modelos para tratativas futuras. Aqui, apenas padronizaremos as fontes que iremos utilizar, a nomeclatura das colunas que vamos chamar e a tratativa minima do formato dos dados como, **datetime** para **date**.

A fun√ß√£o **source** atribuida em ambos os arquivos _.sql_ √© preferencialmente usada em vez de uma refer√™ncia de banco de dados codificada, por criar uma depend√™ncia entre o objeto de banco de dados de origem e o **modelos de teste**. Isso ser√° muito importante quando dermos uma olhada em nossa linhagem de dados. Definir fontes e referenci√°-las com a fun√ß√£o **source** tamb√©m permite testar e documentar essas fontes como voc√™ pode fazer com qualquer outro modelo em seu projeto que voc√™ construa sobre suas fontes. Al√©m disso, se a sua fonte alterar o banco de dados ou o esquema, voc√™ s√≥ precisar√° atualiz√°-lo no seu arquivo **tpch_sources.yml**, em vez de atualizar todos os modelos nos quais ele pode ser usado.

Agora com os **modelos de testes** criados, √© necess√°rio construirmos eles no Snowflake, pois, at√© ent√£o, estamos executando as modifica√ß√µes localmente. Para isso, iremos executar na linha de comando o ```dbt run``` para executar todos os modelos do nosso projeto, que inclui os dois novos **modelos de teste** e os **modelos de exemplo** existentes na pasta **models/example**.

![image](https://github.com/Banco-Mercantil/analytics_code_dbt_snowflake/assets/88452990/e252982c-77c5-4569-852a-56c4000cf12a)

Volte ao Snowflake, atualize os objetos do banco de dados, abra o esquema de desenvolvimento **DBT_PFERNANDES** para confirmar se os novos modelos est√£o nas pastas de **table** e **view**. 

![image](https://github.com/Banco-Mercantil/analytics_code_dbt_snowflake/assets/88452990/33700fea-2fc8-426f-b819-d5d7a66c1370)

Agora certifique-se de submeter seus novos modelos ao seu **branch git**. Clique no bot√£o **Commit and sync** e envie uma mensagem ao seu commit **sources and staging** antes de prosseguir.

A pr√≥xima fase envolve a transforma√ß√£o dos dados. Aqui, s√£o criadas as tabelas unidas que geram uma infinidade de l√≥gica de n√©gocio e insights integrados em um pipeline que preparamos para alimentar uma camada dupla. A camada que criar√° muitos insights e, geralmente, √© o que o usu√°rio final consome se integrarmos ao BI.

O plano aqui √© construir dois **modelos transformados**: um √© um **modelo intermedi√°rio**, que n√£o ser√° a vis√£o final, para executar c√°lculos de novos itens de linha e o outro, ser√° de fato chamado tabela, √© um **modelo de fato** que agrega os c√°lculos de novos itens de linha no n√≠vel do pedido.

O processo de cria√ß√£o ser√° similar ao que fizemos quando criamos os **modelos teste**, por√©m, faremos isso na pasta **marts**. Vamos come√ßar criando um novo arquivo chamado **int_order_items.sql** com o seguinte caminho de arquivo: _models/marts/core/int_order_items.sql_. Em seguida, copie e cole o seguinte bloco de c√≥digo no novo modelo e clique em salvar:

```
with orders as (
    select * from {{ ref('stg_tpch_orders') }}
),

line_item as (
    select * from {{ ref('stg_tpch_line_items') }}
)

select 
    line_item.order_item_key,
    orders.order_key,
    orders.customer_key,
    orders.order_date,
    orders.status_code as order_status_code,
    line_item.part_key,
    line_item.supplier_key,
    line_item.return_flag,
    line_item.line_number,
    line_item.status_code as order_item_status_code,
    line_item.ship_date,
    line_item.commit_date,
    line_item.receipt_date,
    line_item.ship_mode,
    line_item.extended_price,
    line_item.quantity,
    
    -- extended_price is actually the line item total,
    -- so we back out the extended price per item
    (line_item.extended_price/nullif(line_item.quantity, 0))::decimal(16,2) as base_price,
    line_item.discount_percentage,
    (base_price * (1 - line_item.discount_percentage))::decimal(16,2) as discounted_price,

    line_item.extended_price as gross_item_sales_amount,
    (line_item.extended_price * (1 - line_item.discount_percentage))::decimal(16,2) as discounted_item_sales_amount,
    -- We model discounts as negative amounts
    (-1 * line_item.extended_price * line_item.discount_percentage)::decimal(16,2) as item_discount_amount,
    line_item.tax_rate,
    ((gross_item_sales_amount + item_discount_amount) * line_item.tax_rate)::decimal(16,2) as item_tax_amount,
    (
        gross_item_sales_amount + 
        item_discount_amount + 
        item_tax_amount
    )::decimal(16,2) as net_item_sales_amount

from
    orders
inner join line_item
        on orders.order_key = line_item.order_key
order by
    orders.order_date
```

Na parte superior, selecionamos todos os dados dos **modelos de teste** em dois cte's (Common Table Expression/Sub fun√ß√£o) usando a fun√ß√£o ```ref```. A instru√ß√£o principal ```select``` √© unir os dois **modelos de teste**, extrair um peda√ßo de colunas existentes e, em seguida, realizar v√°rios c√°lculos diferentes nos dados dos itens de linha. Todos os c√°lculos s√£o pontos de dados que nos interessam e n√£o foram calculados em nossa fonte de dados brutos, como pre√ßos de itens individuais com descontos e valores totais de vendas com impostos.

_```NULLIS```: esta fun√ß√£o √© utilizada para compara√ß√£o de dois par√¢metros, sendo os dois iguais o retorno ser√° null, caso contr√°rio o primeiro par√¢metro ser√° retornado._

A fun√ß√£o ```ref``` √© a fun√ß√£o mais importante em _DBT_ e √© semelhante √† fun√ß√£o de origem que usamos anteriormente. Neste caso, est√° fazendo refer√™ncia aos nossos modelos de teste. Como regra, voc√™ deve sempre usar a fun√ß√£o ```ref``` para se referir a qualquer modelo DBT existente ao construir modelos. Isso √© importante para criar depend√™ncias entre modelos _DBT_, bem como permitir que voc√™ promova c√≥digo perfeitamente entre diferentes ambientes. Quando promovermos nosso c√≥digo do ambiente de desenvolvimento para o ambiente de produ√ß√£o posteriormente, a fun√ß√£o ```ref``` compilar√° o objeto de banco de dados correto associado ao ambiente de produ√ß√£o com base em nossas configura√ß√µes (tanto na conex√£o quanto no projeto).

Agora vamos criar nosso **modelo de fato** final transformado. Comece criando um novo arquivo chamado **fct_orders.sql** com o seguinte caminho de arquivo:_models/marts/core/fct_orders.sql_. Em seguida, copie e cole o seguinte c√≥digo no arquivo antes de salvar:

```
with orders as (
    select * from {{ ref('stg_tpch_orders') }} 
),

order_item as (
    select * from {{ ref('int_order_items') }}
),

order_item_summary as (
    select 
        order_key,
        sum(gross_item_sales_amount) as gross_item_sales_amount,
        sum(item_discount_amount) as item_discount_amount,
        sum(item_tax_amount) as item_tax_amount,
        sum(net_item_sales_amount) as net_item_sales_amount
    from order_item
    group by 1
),

final as (
    select 
        orders.order_key, 
        orders.order_date,
        orders.customer_key,
        orders.status_code,
        orders.priority_code,
        orders.clerk_name,
        orders.ship_priority,
                
        1 as order_count,                
        order_item_summary.gross_item_sales_amount,
        order_item_summary.item_discount_amount,
        order_item_summary.item_tax_amount,
        order_item_summary.net_item_sales_amount
    from orders
	inner join order_item_summary
            on orders.order_key = order_item_summary.order_key
)

select * from final
order by order_date

```

Este modelo come√ßa trazendo nossos dados de pedidos **(orders)** tempor√°rios **stg_tpch_orders.sql** **(modelo de teste)** e nossos dados de itens de pedido **transformados** **int_order_items** **(modelo intermedi√°rio)** usando a fun√ß√£o ```ref``` em cada caso. 

Em seguida, ele sumariza os c√°lculos dos itens do pedido e agrega esses valores no n√≠vel do pedido antes de junt√°-los novamente aos demais atributos do n√≠vel do pedido para obter a sa√≠da final transformada. O resultado √© que podemos gerar relat√≥rios sobre os pedidos com os descontos e valores de impostos correspondentes, o que n√£o conseguimos fazer apenas com os dados de prepara√ß√£o.

_```GROUP BY 1```: significa agrupar pela primeira coluna do seu conjunto de resultados, independentemente do nome. Voc√™ pode fazer o mesmo com ORDER BY._

Agora que constru√≠mos os **modelos transformados**, vamos fazer executar o ```dbt run``` para incorporar esses modelos em nossos esquemas de desenvolvimento **DBT_PFERNANDES** no Snowflake. Desta vez, em vez de fazer ```dbt run``` e executar todos os modelos do nosso projeto, vamos dizer ao _DBT_ para construir apenas nossos novos modelos transformados. Para fazer isso usaremos o seguinte comando: ```dbt run --select int_order_items+```.

J√° usamos o argumento ```--select``` para dizer ao _DBT_ para executar especificamente o modelo ou caminho que fornecemos ap√≥s o argumento, neste caso, o nosso modelo **int_order_items**. 
O sinal de mais **(+)** anexado ao final de ```int_order_items``` √© um operador gr√°fico que usa o gr√°fico de depend√™ncia do _DBT_ para executar todos os modelos downstream do modelo selecionado. Portanto, neste caso, o _DBT_ ser√° executado ```int_order_items``` e todas as depend√™ncias downstream desse modelo tamb√©m, e √© assim que este comando executa nosso **modelo intermedi√°rio** e o **modelo final**.

Agora vamos dar uma olhada nos resultados detalhados do nosso modelo **fct_orders** para que possamos entender o que est√° acontecendo no c√≥digo compilado. Quando criamos nosso arquivo **dbt_project.yml**, configuramos todos os modelos na pasta **marts** para serem executados usando o warehouse **pc_dbt_wh_large**, e podemos ver isso em a√ß√£o no topo do log:

![image](https://github.com/Banco-Mercantil/analytics_code_dbt_snowflake/assets/88452990/ea31815d-1c03-428a-b536-c421b4aa3e39)

H√° tamb√©m algumas pe√ßas sobre o pr√≥prio modelo a serem destacadas. A primeira √© que o _DBT_ est√° agrupando nossa instru√ß√£o ```select``` em _DDL_ para n√≥s e construindo a tabela em nosso esquema de desenvolvimento **DBT_PFERNANDES**. A configura√ß√£o ```materialized``` em nosso arquivo **dbt_project.yml** √© respons√°vel por construir isso como uma tabela, em oposi√ß√£o √† materializa√ß√£o da visualiza√ß√£o padr√£o.

![image](https://github.com/Banco-Mercantil/analytics_code_dbt_snowflake/assets/88452990/0638138f-1bda-40c0-993a-fea8bd45439e)

O outro ponto importante aqui √© como as instru√ß√µes ```ref``` em cada um dos dois primeiros cte **(sub fun√ß√£o)** foram compiladas no objeto de banco de dados apropriado. Aqui voc√™ pode ver que o _DBT_ compilou o c√≥digo para referenciar o modelo **stg_tpch_orders** e o modelo **int_order_items** no esquema de desenvolvimento **DBT_PFERNANDES**, visto que estamos construindo o modelo em nosso ambiente de desenvolvimento.

Agora reserve um momento para clicar no bot√£o **commit** e enviar uma mensagem **create intermediate and fact models** para salvar suas altera√ß√µes mais recentes.

Nesta fase que constru√≠mos nossos modelos e transforma√ß√µes, √© importante document√°-los e test√°-los. Os recursos nativos do _dbt_ incluem uma estrutura de documenta√ß√£o e teste de dados em duas estruturas: **gen√©rico** e **singular**.

_Singular: √© o teste em sua forma mais simples. Se voc√™ pode escrever uma consulta SQL que retorna linhas com falha, √© um teste dbt que pode ser executado pelo comando "dbt test"._

_Gen√©rico: possuem a capacidade de aceitar argumentos. Voc√™ os define em um bloco de teste e pode referenci√°-los por nome em seus arquivos .yml (aplicando-os a modelos, colunas, fontes, instant√¢neos e sementes). O dbt vem com quatro testes gen√©ricos que funcionam imediatamente: unique, not null, accepted values, e relationships._

**Testes gen√©ricos** e descri√ß√µes para documenta√ß√£o s√£o definidos em arquivos _YAML_. Para come√ßar, criaremos um novo arquivo chamado **core.yml** com o seguinte caminho de arquivo: _models/marts/core/core.yml_. Copie e cole o seguinte bloco de c√≥digo no novo arquivo e salve:

```
version: 2

models:
  - name: fct_orders
    description: orders fact table
    columns:
      - name: order_key
        description: primary key of the model
        tests:
          - unique
          - not_null
          - relationships:
              to: ref('stg_tpch_orders')
              field: order_key
              severity: warn
      - name: customer_key
        description: foreign key for customers
      - name: order_date
        description: date of the order
      - name: status_code
        description: status of the order
        tests:
          - accepted_values:
              values: ['P','O','F']
      - name: priority_code
        description: code associated with the order
      - name: clerk_name
        description: id of the clerk
      - name: ship_priority
        description: numeric representation of the shipping priority, zero being the default
      - name: order_count
        description: count of order
      - name: gross_item_sales_amount
        description: '{{ doc("gross_item_sales_amount") }}'
      - name: item_discount_amount
        description: item level discount amount. this is always less than or equal to zero
      - name: item_tax_amount
        description: item level tax total
      - name: net_item_sales_amount
        description: the net total which factors in discount and tax
```

Este arquivo est√° no diret√≥rio _models/marts/core_ cont√©m testes e descri√ß√µes para os modelos neste diret√≥rio, neste caso, para **fct_orders**. Como pr√°tica recomendada, recomendamos ter pelo menos um arquivo _YAML_ para teste e documenta√ß√£o por diret√≥rio.

Ao observar a formata√ß√£o, o n√≠vel superior √© o modelo que estamos descrevendo e fornecemos todas as informa√ß√µes diretamente abaixo do nome do modelo. No nosso caso, fornecemos uma descri√ß√£o para o nosso modelo.

Descendo para o n√≠vel da coluna, estamos utilizando todos os **testes gen√©ricos** em uma variedade de colunas. Esperamos que a coluna __order_key__ seja ```unique``` e ```not_null```. O teste ```relationships``` est√° verificando se o **order_key** neste modelo tamb√©m existe como um _ID_ no **stg_tpch_orders**. E, finalmente, h√° um teste ```accepted values``` na coluna _status_code_ para garantir que os valores gerados nessa coluna atendam √†s nossas expectativas.

Sobre as descri√ß√µes, elas est√£o divididas entre duas nota√ß√µes diferentes, uma usando texto simples e outra usando uma **refer√™ncia jinja** a um bloco de documento. A primeira funciona exatamente como no n√≠vel do modelo e permite documentar qualquer coluna no arquivo _YAML_. O bloco _doc_ permite escrever descri√ß√µes em arquivos **markdown**.

**Arquivos markdown**: para construir o arquivo **markdown** para acompanhar a descri√ß√£o do bloco de documentos, crie um novo arquivo chamado **core.md** com o seguinte caminho de arquivo: _models/marts/core/core.md_. Depois, copie e cole o trecho a seguir:

```
# the below are descriptions from fct_orders

{% docs gross_item_sales_amount %} same as extended_price {% enddocs %}
```

Este arquivo **markdown** possui um bloco _doc_ **({% docs gross_item_sales_amount %})** correspondente a uma coluna, na qual, sua descri√ß√£o no arquivo _YAML_ **('{{ doc("gross_item_sales_amount") }}')** √© este bloco _doc_ no **markdown**. 

O nome entre aspas na descri√ß√£o do bloco de documento _YAML_ se conecta ao nome no bloco de documento de abertura em nosso arquivo **markdown**. Nesse caso, **'{{doc("gross_item_sales_amount") }}'** na descri√ß√£o _YAML_ corresponde **{% docs gross_item_sales_amount %}** em nosso arquivo **markdown**. A partir da√≠, a descri√ß√£o do texto em nosso arquivo **markdown** √© o que √© passado como descri√ß√£o em nosso arquivo _YAML_.

Construir descri√ß√µes com blocos de documentos em arquivos **markdown** oferece a capacidade de formatar suas descri√ß√µes com **markdown** e √© √∫til ao construir descri√ß√µes longas. Al√©m disso, se uma coluna existir em v√°rios modelos com a mesma descri√ß√£o, poder√° reutilizar o mesmo bloco de documentos para essa coluna.

No arquivo **core.yml**, a coluna **item_discount_amount** cont√©m a restri√ß√£o de que os valores dessa coluna deve ser sempre menor ou igual a zero. Para averiguar podemos usar o **teste singular**. 

Para isso precisaremos de um **teste de dados personalizado**, ent√£o come√ßaremos criando um novo arquivo chamado **fct_orders_negative_discount_amount.sql** com o seguinte caminho de arquivo: _tests/fct_orders_negative_discount_amount.sql_. Copie e cole o seguinte c√≥digo no novo arquivo antes de salvar:

```
--If no discount is given it should be equal to zero 
--Otherwise it will be negative

select * from {{ ref('fct_orders') }}  
 where item_discount_amount > 0

```

Os **testes personalizados** s√£o escritos como instru√ß√µes ```select``` _SQL_ , onde um teste com falha retornar√° os registros com falha como sa√≠da. Isso significa que para que o teste seja aprovado, o resultado n√£o retornar√° nenhuma linha e precisaremos escrever o teste com essa expectativa em mente. Dado que a nossa suposi√ß√£o sobre o **item_discount_amount** √© que ser√° sempre menor ou igual a zero, queremos ent√£o escrever o teste para procurar quando o valor √© maior que zero.

Para executar este teste no arquivo _YAML_ **(core.yaml)**, o _DBT_ sabe quando executar atrav√©s da combina√ß√£o do arquivo que est√° no diret√≥rio **tests** (conforme definido em nosso arquivo **dbt_project.yml** ) e a instru√ß√£o ```select``` usando a fun√ß√£o ```ref``` para criar uma depend√™ncia com o modelo apropriado. Dessa forma, sempre que voc√™ usar o comando ```dbt test``` para testar esse modelo, esse teste ser√° executado junto com todos os outros testes em n√≠vel de coluna.

Para fazer os teste, voc√™ pode inserir o seguinte comando na linha de comando: ```dbt test --select fct_orders```. Semelhante √†s execu√ß√µes de nosso modelo, o c√≥digo compilado que √© passado ao Snowflake para cada teste √© vis√≠vel na se√ß√£o **Details** de resultados do teste.

![image](https://github.com/Banco-Mercantil/analytics_code_dbt_snowflake/assets/88452990/b31d2789-c19b-4fc3-803a-87065ef2184e)

Sobre a documenta√ß√£o, o comando ```dbt docs generate``` ir√° dizer ao _DBT_ para criar os documentos. Ap√≥s executar este comando, um √≠cone de livro ficar√° dispon√≠vel na _IDE_ do _dbt_ ao lado do nome do seu **branch** (snowflake_workshop), na lateral esquerda superior. 

![image](https://github.com/Banco-Mercantil/analytics_code_dbt_snowflake/assets/88452990/dd08bd6f-d944-4f79-9102-055725da846b)

O site de documenta√ß√£o ser√° lan√ßado em uma nova guia. Depois de carregar, entre na barra de pesquisa superior, digite **fct_orders** e clique no resultado superior para obter a documenta√ß√£o desse modelo.

Quando isso for carregado, voc√™ poder√° ver nossa descri√ß√£o em n√≠vel de modelo, as descri√ß√µes em n√≠vel de coluna que escrevemos no arquivo _YAML_ e a descri√ß√£o que veio dos blocos de documentos.

Agora reserve um momento para clicar no bot√£o **commit** e enviar uma mensagem **tests and docs** para salvar suas altera√ß√µes mais recentes.

At√© este ponto, todo o trabalho que fizemos no _dbt Cloud IDE_ foi em nosso ambiente de desenvolvimento, com o c√≥digo salvo em uma ramifica√ß√£o de desenvolvimento e os modelos que constru√≠mos criados em nosso esquema de desenvolvimento no Snowflake, conforme definido em nossa conex√£o com o ambiente desenvolvimento.

Agora que conclu√≠mos os testes e a documenta√ß√£o do nosso trabalho, estamos prontos para implantar nosso c√≥digo do ambiente de desenvolvimento para o ambiente de produ√ß√£o e isso envolve duas etapas:

_1.0 Passando o c√≥digo do nosso **branch de desenvolvimento** para o **branch de produ√ß√£o** (branch principal) em nosso reposit√≥rio. H√° um processo de revis√£o a ser realizado antes de mesclar o c√≥digo com o **branch principal** de um reposit√≥rio._
	
_2.0 Implantando c√≥digo em nosso ambiente de produ√ß√£o. Depois que nosso c√≥digo for mesclado ao **branch principal**, precisaremos executar o dbt em nosso ambiente de produ√ß√£o para construir todos os nossos modelos e executar todos os nossos testes. Felizmente, o fluxo do **partner connect** j√° criou nosso ambiente de implanta√ß√£o e trabalho para facilitar._
	
Para a primeira etapa, depois que todo o seu trabalho estiver confirmado, o bot√£o de fluxo de trabalho do git aparecer√° agora como **Merge to main**. Clique **Merge to main** e o processo de mesclagem ser√° executado automaticamente. Quando estiver conclu√≠do, voc√™ dever√° ver o bot√£o lido **Create branche** o **branch** que voc√™ est√° vendo no momento se tornar√° **main**.

Para a segunda etapa, agora que todo o nosso trabalho de desenvolvimento foi mesclado ao **branch principal**, podemos construir nosso trabalho de implanta√ß√£o. Dado que nosso ambiente de produ√ß√£o e trabalho de produ√ß√£o foram criados automaticamente para n√≥s por meio do **Partner Connect**, tudo o que precisamos fazer aqui √© atualizar algumas configura√ß√µes padr√£o para atender √†s nossas necessidades.

Clique na guia **Deploy** na barra superior e, em seguida, clique em **Environments**. Voc√™ dever√° ver dois ambientes listados e dever√° clicar no ambiente **Deployment**  para abri-lo e, em seguida, **Settings** no canto superior direito para modific√°-lo.

Nosso trabalho de implanta√ß√£o ser√° constru√≠do em nosso **PC_DBT_DBbanco** de dados e usar√° a fun√ß√£o e o armaz√©m padr√£o do **Partner Connect** para fazer isso. 

A se√ß√£o de **credenciais de implanta√ß√£o** tamb√©m usa as informa√ß√µes que foram criadas em nosso trabalho do **Partner Connect** para criar a conex√£o de credenciais. No entanto, ele est√° usando o mesmo esquema padr√£o que estamos usando como esquema para nosso **ambiente de desenvolvimento**. 

Vamos atualizar o esquema para criar um novo esquema especificamente para nosso **ambiente de produ√ß√£o**. Clique no bot√£o **Edit**, no canto superior direito, para permitir que voc√™ modifique os valores dos campos existentes. Role para baixo at√© o campo **schema** na se√ß√£o **Deployment Credentials** e atualize o nome do esquema para **production**. Certifique-se de clicar **Save**, no canto superior direito, depois de fazer a altera√ß√£o.

![image](https://github.com/Banco-Mercantil/analytics_code_dbt_snowflake/assets/88452990/e02ccc22-7aec-409e-bdb2-950fed572254)

Ao atualizar o esquema de nosso **ambiente de produ√ß√£o** para **production**, ele garante que nosso trabalho de implanta√ß√£o para esse ambiente construir√° nossos modelos _dbt_ no esquema **production** do banco de dados **PC_DBT_DB**, conforme definido na se√ß√£o **Snowflake Connection**.

Agora vamos passar para o nosso trabalho de produ√ß√£o. Clique na guia **Deploy** na barra superior e, em seguida, clique em **Jobs**. Voc√™ dever√° ver um arquivo **Partner Connect Trial Job**. Clique nele e selecione **Settings** para modific√°-lo.

A se√ß√£o **Ambiente** √© o que conecta este trabalho ao ambiente em que queremos que ele seja executado. Este trabalho j√° est√° padronizado para usar o ambiente de implanta√ß√£o que acabamos de atualizar e o restante das configura√ß√µes que podemos manter como est√£o a op√ß√£o de gerar documentos, executar atualiza√ß√£o de origem e adiar para um estado de execu√ß√£o anterior. 

Para os prop√≥sitos do nosso laborat√≥rio, vamos nos limitar apenas √† gera√ß√£o de documentos. A se√ß√£o **Comandos** √© onde especificamos exatamente quais comandos queremos executar durante este trabalho.

Aqui voc√™ ver√° a seguinte estrutura: ```dbt seed```, ```dbt run```, ```dbt test```. Para este trabalho, iremos excluir ```dbt seed```, restando apenas os demais. 

Isso √© feito, pois este projeto n√£o possui uma semente, para projetos que possuem, as configura√ß√µes dever√£o serem mantidas. Pois, queremos que nossa semente seja carregada primeiro, depois execute nossos modelos e, finalmente, teste-os. A ordem disso tamb√©m √© importante, considerando que precisamos que nossa semente seja criada antes de podermos executar nosso modelo incremental, e precisamos que nossos modelos sejam criados antes de podermos test√°-los. 

Finalmente, temos a se√ß√£o **Triggers**, onde temos uma s√©rie de op√ß√µes diferentes para agendar nosso trabalho. Dado que nossos dados n√£o s√£o atualizados regularmente aqui e estamos executando esse trabalho manualmente por enquanto, tamb√©m deixaremos esta se√ß√£o de lado.

Nesta fase iremos alterar apenas o nome. Clique em **Edit**, no canto superior direito do trabalho, para permitir altera√ß√µes. Em seguida, atualize o nome do trabalho para **Production Job** , na se√ß√£o **General Settings**, para indic√°-lo como nosso trabalho de implanta√ß√£o de produ√ß√£o. Depois de fazer isso, clique **Save** no topo da tela.

Agora vamos executar nosso trabalho. Clicar no nome do trabalho no caminho na parte superior da tela o levar√° de volta √† p√°gina do hist√≥rico de execu√ß√£o do trabalho, onde voc√™ poder√° clicar **Run now** para iniciar o trabalho.

![image](https://github.com/Banco-Mercantil/analytics_code_dbt_snowflake/assets/88452990/291bf63d-a262-4ad2-b397-4488eefd2d90)

Ao terminar voc√™ notar√° que o trabalho apresentou um erro, o que √© estranho considerando que n√£o havia n√£o houve problemas quando executamos nossos trabalhos e testes anteriormente. Olhando mais de perto, podemos ver que h√° um problema na etapa de teste e queremos clicar nessa etapa para expandir os logs e ver o que est√° acontecendo.

![image](https://github.com/Banco-Mercantil/analytics_code_dbt_snowflake/assets/88452990/4ec19b8d-07f7-4cc4-876b-9832956d4af9)

Um dos testes dos nossos modelos de exemplo falhou! Isso foi um pouco inesperado, mas olhando todos os logs novamente podemos ver que os modelos que criamos foram constru√≠dos com sucesso no Snowflake e os testes para esses modelos foram aprovados. Neste ponto, podemos seguir em frente e corrigir o teste de exemplo com falha outro dia. Se preferir, podemos excluir os modelos de testes e rodar novamente os comandos para n√£o houver erros.

Vamos ao Snowflake para confirmar se tudo foi constru√≠do conforme esperado em nosso esquema de produ√ß√£o. Atualize os objetos de banco de dados em sua conta Snowflake e voc√™ dever√° ver o esquema **production** agora em nosso banco de dados padr√£o do **Partner Connect**. Se voc√™ clicar no esquema e tudo funcionar com sucesso, voc√™ poder√° ver todos os modelos que desenvolvemos l√°.

![image](https://github.com/Banco-Mercantil/analytics_code_dbt_snowflake/assets/88452990/5f7bb8ba-8b75-4c9a-b635-a99def83be66)
